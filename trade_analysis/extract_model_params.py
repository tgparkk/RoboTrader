#!/usr/bin/env python3
"""
í•™ìŠµëœ ML ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í•˜ë“œì½”ë”© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
- í”¼í´ íŒŒì¼ì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
- Python ì½”ë“œë¡œ ë³€í™˜í•˜ì—¬ ê²½ëŸ‰í™”ëœ ì˜ˆì¸¡ê¸° ìƒì„±
"""

import os
import sys
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

def extract_random_forest_params(model) -> Dict[str, Any]:
    """RandomForest ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    try:
        params = {
            'n_estimators': model.n_estimators,
            'max_depth': model.max_depth,
            'min_samples_split': model.min_samples_split,
            'min_samples_leaf': model.min_samples_leaf,
            'n_features': model.n_features_in_,
            'n_classes': getattr(model, 'n_classes_', None),
            'trees': []
        }
        
        # ê° íŠ¸ë¦¬ì˜ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        for i, tree in enumerate(model.estimators_[:min(5, len(model.estimators_))]):  # ì²˜ìŒ 5ê°œë§Œ
            tree_info = {
                'tree_id': i,
                'feature_importances': tree.feature_importances_.tolist(),
                'n_nodes': tree.tree_.node_count,
                'max_depth': tree.tree_.max_depth
            }
            params['trees'].append(tree_info)
        
        return params
    except Exception as e:
        return {'error': str(e)}

def extract_xgboost_params(model) -> Dict[str, Any]:
    """XGBoost ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    try:
        params = {
            'n_estimators': model.n_estimators,
            'max_depth': model.max_depth,
            'learning_rate': model.learning_rate,
            'subsample': model.subsample,
            'colsample_bytree': model.colsample_bytree,
            'feature_importances': model.feature_importances_.tolist() if hasattr(model, 'feature_importances_') else None
        }
        
        # ë¶€ìŠ¤í„° ì •ë³´
        if hasattr(model, 'get_booster'):
            booster = model.get_booster()
            params['booster_dump'] = booster.get_dump()[:5]  # ì²˜ìŒ 5ê°œ íŠ¸ë¦¬ë§Œ
        
        return params
    except Exception as e:
        return {'error': str(e)}

def extract_lightgbm_params(model) -> Dict[str, Any]:
    """LightGBM ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    try:
        params = {
            'n_estimators': model.n_estimators,
            'max_depth': model.max_depth,
            'learning_rate': model.learning_rate,
            'subsample': model.subsample,
            'colsample_bytree': model.colsample_bytree,
            'feature_importances': model.feature_importances_.tolist() if hasattr(model, 'feature_importances_') else None
        }
        
        # ë¶€ìŠ¤í„° ì •ë³´
        if hasattr(model, 'booster_'):
            params['model_dump'] = model.booster_.dump_model()
            params['num_trees'] = model.booster_.num_trees()
        
        return params
    except Exception as e:
        return {'error': str(e)}

def extract_all_model_params():
    """ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    model_dir = Path("ml_models")
    
    if not model_dir.exists():
        print("âŒ ML ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    print("í•™ìŠµëœ ML ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì‹œì‘...")
    print("=" * 60)
    
    extracted_params = {}
    
    # ê° ëª¨ë¸ íŒŒì¼ ì²˜ë¦¬
    for model_file in model_dir.glob("*.pkl"):
        if model_file.name in ["scaler.pkl", "label_encoders.pkl"]:
            continue
            
        model_name = model_file.stem
        print(f"\n[ëª¨ë¸] {model_name} ë¶„ì„ ì¤‘...")
        
        try:
            with open(model_file, 'rb') as f:
                model = pickle.load(f)
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            if 'random_forest' in model_name or 'rf' in model_name:
                params = extract_random_forest_params(model)
            elif 'xgb' in model_name or 'xgboost' in model_name:
                params = extract_xgboost_params(model)
            elif 'lgb' in model_name or 'lightgbm' in model_name:
                params = extract_lightgbm_params(model)
            else:
                params = {'error': f'ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {type(model)}'}
            
            extracted_params[model_name] = params
            
            if 'error' in params:
                print(f"   [ì‹¤íŒ¨] íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì‹¤íŒ¨: {params['error']}")
            else:
                print(f"   [ì„±ê³µ] íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì„±ê³µ")
                if 'n_estimators' in params:
                    print(f"      íŠ¸ë¦¬ ê°œìˆ˜: {params['n_estimators']}")
                if 'max_depth' in params:
                    print(f"      ìµœëŒ€ ê¹Šì´: {params['max_depth']}")
                if 'feature_importances' in params and params['feature_importances']:
                    top_features = sorted(enumerate(params['feature_importances']), 
                                        key=lambda x: x[1], reverse=True)[:5]
                    print(f"      ì£¼ìš” íŠ¹ì„±: {[f'F{i}({v:.3f})' for i, v in top_features]}")
        
        except Exception as e:
            print(f"   âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            extracted_params[model_name] = {'error': str(e)}
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ ì¶”ì¶œ
    print(f"\n[ì „ì²˜ë¦¬] ìŠ¤ì¼€ì¼ëŸ¬ ë¶„ì„...")
    scaler_path = model_dir / "scaler.pkl"
    if scaler_path.exists():
        try:
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            
            scaler_params = {
                'type': type(scaler).__name__,
                'mean': scaler.mean_.tolist() if hasattr(scaler, 'mean_') else None,
                'scale': scaler.scale_.tolist() if hasattr(scaler, 'scale_') else None,
                'n_features': scaler.n_features_in_ if hasattr(scaler, 'n_features_in_') else None
            }
            extracted_params['scaler'] = scaler_params
            print(f"   [ì„±ê³µ] ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„° ì¶”ì¶œ: {scaler_params['type']}")
            
        except Exception as e:
            print(f"   [ì‹¤íŒ¨] ìŠ¤ì¼€ì¼ëŸ¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # ë¼ë²¨ ì¸ì½”ë” ì •ë³´ ì¶”ì¶œ
    encoders_path = model_dir / "label_encoders.pkl"
    if encoders_path.exists():
        try:
            with open(encoders_path, 'rb') as f:
                encoders = pickle.load(f)
            
            encoder_params = {}
            for name, encoder in encoders.items():
                encoder_params[name] = {
                    'classes': encoder.classes_.tolist() if hasattr(encoder, 'classes_') else None
                }
            
            extracted_params['label_encoders'] = encoder_params
            print(f"   [ì„±ê³µ] ë¼ë²¨ ì¸ì½”ë” íŒŒë¼ë¯¸í„° ì¶”ì¶œ: {len(encoder_params)}ê°œ")
            
        except Exception as e:
            print(f"   [ì‹¤íŒ¨] ë¼ë²¨ ì¸ì½”ë” ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = "extracted_model_params.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('#!/usr/bin/env python3\n')
        f.write('"""\n')
        f.write('ì¶”ì¶œëœ ML ëª¨ë¸ íŒŒë¼ë¯¸í„°\n')
        f.write('ìë™ ìƒì„±ë¨ - ìˆ˜ë™ í¸ì§‘í•˜ì§€ ë§ˆì„¸ìš”\n')
        f.write('"""\n\n')
        f.write('import numpy as np\n\n')
        f.write('# ì¶”ì¶œëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°\n')
        f.write(f'EXTRACTED_PARAMS = {repr(extracted_params)}\n\n')
        
        # ê°„ë‹¨í•œ ì ‘ê·¼ í•¨ìˆ˜ë“¤
        f.write('''
def get_model_params(model_name: str):
    """ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°íšŒ"""
    return EXTRACTED_PARAMS.get(model_name, {})

def get_feature_importances(model_name: str):
    """íŠ¹ì„± ì¤‘ìš”ë„ ì¡°íšŒ"""
    params = get_model_params(model_name)
    return params.get('feature_importances', [])

def get_scaler_params():
    """ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„° ì¡°íšŒ"""
    return EXTRACTED_PARAMS.get('scaler', {})

def get_top_features(model_name: str, top_k: int = 10):
    """ì£¼ìš” íŠ¹ì„± ì¸ë±ìŠ¤ ì¡°íšŒ"""
    importances = get_feature_importances(model_name)
    if not importances:
        return []
    
    indexed = list(enumerate(importances))
    sorted_features = sorted(indexed, key=lambda x: x[1], reverse=True)
    return [idx for idx, _ in sorted_features[:top_k]]

if __name__ == "__main__":
    print("ğŸ“Š ì¶”ì¶œëœ ML ëª¨ë¸ ì •ë³´")
    print("=" * 40)
    
    for model_name, params in EXTRACTED_PARAMS.items():
        if 'error' in params:
            continue
        print(f"\\nğŸ¤– {model_name}")
        
        if model_name == 'scaler':
            print(f"   íƒ€ì…: {params.get('type')}")
            print(f"   íŠ¹ì„± ìˆ˜: {params.get('n_features')}")
        elif model_name == 'label_encoders':
            print(f"   ì¸ì½”ë” ìˆ˜: {len(params)}")
        else:
            if 'n_estimators' in params:
                print(f"   íŠ¸ë¦¬ ê°œìˆ˜: {params['n_estimators']}")
            if 'max_depth' in params:
                print(f"   ìµœëŒ€ ê¹Šì´: {params['max_depth']}")
            
            top_features = get_top_features(model_name, 5)
            if top_features:
                print(f"   ì£¼ìš” íŠ¹ì„±: {top_features}")
''')
    
    print(f"\n[ì €ì¥] ì¶”ì¶œëœ íŒŒë¼ë¯¸í„° ì €ì¥: {output_path}")
    print("=" * 60)
    
    # ìš”ì•½ í†µê³„
    total_models = len([k for k in extracted_params.keys() 
                      if k not in ['scaler', 'label_encoders'] and 'error' not in extracted_params[k]])
    print(f"[ì™„ë£Œ] ì¶”ì¶œ ì™„ë£Œ: {total_models}ê°œ ëª¨ë¸")
    
    if total_models > 0:
        print("\n[ë‹¤ìŒë‹¨ê³„]")
        print("1. python trade_analysis/extracted_model_params.py  # ì¶”ì¶œ ê²°ê³¼ í™•ì¸")
        print("2. python trade_analysis/create_hardcoded_predictor.py  # ê²½ëŸ‰í™” ì˜ˆì¸¡ê¸° ìƒì„±")

if __name__ == "__main__":
    extract_all_model_params()