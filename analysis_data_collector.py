#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë§¤ë§¤ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ê¸°
- ë‚ ì§œë³„ í›„ë³´ ì¢…ëª© ì¡°íšŒ (DB ë˜ëŠ” ì‹ í˜¸ ë¡œê·¸ì—ì„œ)
- ë¶„ë´‰/ì¼ë´‰ ë°ì´í„° ìë™ ìˆ˜ì§‘ (ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ API í˜¸ì¶œ)
- ì™„ì „í•œ ë°ì´í„°ì…‹ êµ¬ì„±
"""

import os
import sys
import sqlite3
import pickle
import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import logging
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from api.kis_api_manager import KISAPIManager
from utils.korean_time import now_kst
from utils.logger import setup_logger

logger = setup_logger(__name__)

class AnalysisDataCollector:
    """ë§¤ë§¤ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self,
                 db_path: str = "data/robotrader.db",
                 minute_cache_dir: str = "cache/minute_data",
                 daily_cache_dir: str = "cache/daily_data",
                 signal_log_dir: str = "signal_replay_log"):
        self.db_path = db_path
        self.minute_cache_dir = Path(minute_cache_dir)
        self.daily_cache_dir = Path(daily_cache_dir)
        self.signal_log_dir = Path(signal_log_dir)

        # API ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.api_manager = None
        try:
            self.api_manager = KISAPIManager()
            if self.api_manager.initialize():
                logger.info("KIS API ë§¤ë‹ˆì € ì´ˆê¸°í™” ì„±ê³µ")
            else:
                logger.warning("KIS API ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨ - ìºì‹œ ì „ìš© ëª¨ë“œë¡œ ë™ì‘")
                self.api_manager = None
        except Exception as e:
            logger.warning(f"KIS API ë§¤ë‹ˆì € ì´ˆê¸°í™” ì˜¤ë¥˜: {e} - ìºì‹œ ì „ìš© ëª¨ë“œë¡œ ë™ì‘")
            self.api_manager = None

        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.minute_cache_dir.mkdir(parents=True, exist_ok=True)
        self.daily_cache_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - DB: {db_path}")
        logger.info(f"   - ë¶„ë´‰ ìºì‹œ: {self.minute_cache_dir}")
        logger.info(f"   - ì¼ë´‰ ìºì‹œ: {self.daily_cache_dir}")
        logger.info(f"   - ì‹ í˜¸ ë¡œê·¸: {self.signal_log_dir}")
        logger.info(f"   - API ìƒíƒœ: {'í™œì„±' if self.api_manager else 'ë¹„í™œì„± (ìºì‹œë§Œ)'}")

    def get_candidate_stocks_by_date_range(self, start_date: str, end_date: str) -> Dict[str, List[Dict]]:
        """ë‚ ì§œ ë²”ìœ„ì˜ í›„ë³´ ì¢…ëª© ì¡°íšŒ"""
        try:
            # 1. ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ ì‹œë„
            if os.path.exists(self.db_path):
                stocks_from_db = self._get_stocks_from_database(start_date, end_date)
                if stocks_from_db:
                    return stocks_from_db

            # 2. ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì‹ í˜¸ ë¡œê·¸ì—ì„œ ì¶”ì¶œ
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹ í˜¸ ë¡œê·¸ì—ì„œ ì¢…ëª© ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
            return self._extract_stocks_from_logs(start_date, end_date)

        except Exception as e:
            logger.error(f"í›„ë³´ ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def _get_stocks_from_database(self, start_date: str, end_date: str) -> Dict[str, List[Dict]]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í›„ë³´ ì¢…ëª© ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ë¨¼ì € í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='candidate_stocks'")
                if not cursor.fetchone():
                    logger.info("candidate_stocks í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return {}

                cursor.execute("""
                    SELECT stock_code, stock_name, selection_date, selection_reason
                    FROM candidate_stocks
                    WHERE DATE(selection_date) BETWEEN ? AND ?
                    ORDER BY selection_date, stock_code
                """, (start_date, end_date))

                results = cursor.fetchall()

                if not results:
                    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {start_date}~{end_date} ê¸°ê°„ì˜ í›„ë³´ ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return {}

                # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
                stocks_by_date = {}
                for row in results:
                    stock_code, stock_name, selection_date, selection_reason = row
                    date_str = selection_date.split(' ')[0] if ' ' in selection_date else selection_date

                    if date_str not in stocks_by_date:
                        stocks_by_date[date_str] = []

                    stocks_by_date[date_str].append({
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'selection_date': selection_date,
                        'selection_reason': selection_reason
                    })

                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {start_date}~{end_date} ê¸°ê°„ í›„ë³´ ì¢…ëª© {len(results)}ê°œ ì¡°íšŒ")
                return stocks_by_date

        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def _extract_stocks_from_logs(self, start_date: str, end_date: str) -> Dict[str, List[Dict]]:
        """ì‹ í˜¸ ë¡œê·¸ì—ì„œ ì¢…ëª© ì •ë³´ ì¶”ì¶œ"""
        stocks_by_date = {}

        try:
            # ë‚ ì§œ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ íŒŒì¼ë“¤ ì°¾ê¸°
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')

            for log_file in self.signal_log_dir.glob("signal_new2_replay_*.txt"):
                # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                match = re.search(r'(\d{8})', log_file.name)
                if not match:
                    continue

                file_date_str = match.group(1)
                file_date = datetime.strptime(file_date_str, '%Y%m%d')

                if not (start_dt <= file_date <= end_dt):
                    continue

                # ë¡œê·¸ íŒŒì¼ì—ì„œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
                stocks = self._parse_stocks_from_log_file(log_file, file_date_str)
                if stocks:
                    stocks_by_date[file_date_str] = stocks
                    logger.info(f"{file_date_str}: {len(stocks)}ê°œ ì¢…ëª© ì¶”ì¶œ")

            total_stocks = sum(len(stocks) for stocks in stocks_by_date.values())
            logger.info(f"ì‹ í˜¸ ë¡œê·¸ì—ì„œ ì´ {total_stocks}ê°œ ì¢…ëª© ì¶”ì¶œ")

        except Exception as e:
            logger.error(f"ì‹ í˜¸ ë¡œê·¸ íŒŒì‹± ì‹¤íŒ¨: {e}")

        return stocks_by_date

    def _parse_stocks_from_log_file(self, log_file: Path, date_str: str) -> List[Dict]:
        """ê°œë³„ ë¡œê·¸ íŒŒì¼ì—ì„œ ì¢…ëª© ì •ë³´ ì¶”ì¶œ"""
        stocks = []
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # "=== ì¢…ëª©ì½”ë“œ -" íŒ¨í„´ìœ¼ë¡œ ì¢…ëª© ì¶”ì¶œ
            pattern = r'=== (\d{6}) -'
            matches = re.findall(pattern, content)

            for stock_code in set(matches):  # ì¤‘ë³µ ì œê±°
                stocks.append({
                    'stock_code': stock_code,
                    'stock_name': f"ì¢…ëª©_{stock_code}",
                    'selection_date': date_str,
                    'selection_reason': 'from_log'
                })

        except Exception as e:
            logger.error(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨ ({log_file}): {e}")

        return stocks

    def collect_daily_data(self, stock_code: str, days: int = 60) -> Optional[pd.DataFrame]:
        """ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ API í˜¸ì¶œ)"""
        cache_file = self.daily_cache_dir / f"{stock_code}_daily.pkl"

        # 1. ìºì‹œ í™•ì¸
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)

                # í•œêµ­íˆ¬ìì¦ê¶Œ API í˜•íƒœì¸ì§€ í™•ì¸
                if isinstance(cached_data, pd.DataFrame) and 'stck_bsop_date' in cached_data.columns:
                    # ì»¬ëŸ¼ëª… ë³€í™˜
                    daily_data = cached_data.rename(columns={
                        'stck_bsop_date': 'date',
                        'stck_clpr': 'close',
                        'stck_oprc': 'open',
                        'stck_hgpr': 'high',
                        'stck_lwpr': 'low',
                        'acml_vol': 'volume'
                    })

                    # ë°ì´í„° íƒ€ì… ë³€í™˜
                    numeric_cols = ['close', 'open', 'high', 'low', 'volume']
                    for col in numeric_cols:
                        if col in daily_data.columns:
                            daily_data[col] = pd.to_numeric(daily_data[col], errors='coerce')

                    # ë‚ ì§œ ì¸ë±ìŠ¤ ì„¤ì •
                    daily_data['date'] = pd.to_datetime(daily_data['date'], format='%Y%m%d')
                    daily_data = daily_data.set_index('date').sort_index()

                    # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                    if len(daily_data) >= 10:
                        logger.info(f"[{stock_code}] ìºì‹œëœ ì¼ë´‰ ë°ì´í„° ì‚¬ìš©: {len(daily_data)}ì¼")
                        return daily_data

            except Exception as e:
                logger.warning(f"[{stock_code}] ì¼ë´‰ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 2. API í˜¸ì¶œ
        try:
            if not self.api_manager:
                logger.warning(f"[{stock_code}] API ë§¤ë‹ˆì €ê°€ ì—†ìŠµë‹ˆë‹¤. ìºì‹œëœ ë°ì´í„°ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return None

            logger.info(f"[{stock_code}] APIë¡œ ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

            api_data = self.api_manager.get_daily_data(stock_code, days)

            if api_data is None or api_data.empty:
                logger.warning(f"[{stock_code}] APIì—ì„œ ì¼ë´‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

            # ìºì‹œ ì €ì¥
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(api_data, f)
                logger.info(f"[{stock_code}] ì¼ë´‰ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"[{stock_code}] ì¼ë´‰ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

            # ë°ì´í„° ë³€í™˜ í›„ ë°˜í™˜
            if 'stck_bsop_date' in api_data.columns:
                daily_data = api_data.rename(columns={
                    'stck_bsop_date': 'date',
                    'stck_clpr': 'close',
                    'stck_oprc': 'open',
                    'stck_hgpr': 'high',
                    'stck_lwpr': 'low',
                    'acml_vol': 'volume'
                })

                numeric_cols = ['close', 'open', 'high', 'low', 'volume']
                for col in numeric_cols:
                    if col in daily_data.columns:
                        daily_data[col] = pd.to_numeric(daily_data[col], errors='coerce')

                daily_data['date'] = pd.to_datetime(daily_data['date'], format='%Y%m%d')
                daily_data = daily_data.set_index('date').sort_index()

                logger.info(f"[{stock_code}] API ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(daily_data)}ì¼")
                return daily_data

        except Exception as e:
            logger.error(f"[{stock_code}] API ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return None

    def collect_minute_data(self, stock_code: str, date_str: str) -> Optional[pd.DataFrame]:
        """ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ API í˜¸ì¶œ)"""
        cache_file = self.minute_cache_dir / f"{stock_code}_{date_str}.pkl"

        # 1. ìºì‹œ í™•ì¸
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)

                if isinstance(cached_data, pd.DataFrame) and len(cached_data) > 0:
                    logger.info(f"[{stock_code}] {date_str} ìºì‹œëœ ë¶„ë´‰ ë°ì´í„° ì‚¬ìš©: {len(cached_data)}ë¶„")
                    return cached_data

            except Exception as e:
                logger.warning(f"[{stock_code}] {date_str} ë¶„ë´‰ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 2. API í˜¸ì¶œ
        try:
            if not self.api_manager:
                logger.warning(f"[{stock_code}] {date_str} API ë§¤ë‹ˆì €ê°€ ì—†ìŠµë‹ˆë‹¤. ìºì‹œëœ ë°ì´í„°ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return None

            logger.info(f"[{stock_code}] {date_str} APIë¡œ ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

            # 1ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘
            api_data = self.api_manager.get_minute_data(stock_code, date_str)

            if api_data is None or api_data.empty:
                logger.warning(f"[{stock_code}] {date_str} APIì—ì„œ ë¶„ë´‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

            # ìºì‹œ ì €ì¥
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(api_data, f)
                logger.info(f"[{stock_code}] {date_str} ë¶„ë´‰ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"[{stock_code}] {date_str} ë¶„ë´‰ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

            logger.info(f"[{stock_code}] {date_str} API ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(api_data)}ë¶„")
            return api_data

        except Exception as e:
            logger.error(f"[{stock_code}] {date_str} API ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return None

    def collect_complete_dataset(self, start_date: str, end_date: str) -> Dict[str, Dict]:
        """ì™„ì „í•œ ë°ì´í„°ì…‹ ìˆ˜ì§‘ (ë¶„ë´‰ + ì¼ë´‰)"""
        logger.info(f"=== ì™„ì „í•œ ë°ì´í„°ì…‹ ìˆ˜ì§‘: {start_date} ~ {end_date} ===")

        # 1. í›„ë³´ ì¢…ëª© ì¡°íšŒ
        stocks_by_date = self.get_candidate_stocks_by_date_range(start_date, end_date)

        if not stocks_by_date:
            logger.warning("ìˆ˜ì§‘í•  ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        # 2. ëª¨ë“  ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±
        all_stocks = set()
        for date_stocks in stocks_by_date.values():
            all_stocks.update(stock['stock_code'] for stock in date_stocks)

        logger.info(f"ì´ {len(all_stocks)}ê°œì˜ ê³ ìœ  ì¢…ëª© ë°œê²¬")

        # 3. ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼
        dataset = {}

        for date_str, date_stocks in stocks_by_date.items():
            logger.info(f"\n=== {date_str} ë°ì´í„° ìˆ˜ì§‘ ({len(date_stocks)}ê°œ ì¢…ëª©) ===")

            dataset[date_str] = {
                'stocks': date_stocks,
                'data': {}
            }

            for stock_info in date_stocks:
                stock_code = stock_info['stock_code']
                logger.info(f"[{stock_code}] ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

                stock_data = {
                    'stock_info': stock_info,
                    'daily_data': None,
                    'minute_data': None,
                    'complete': False
                }

                # ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘
                daily_data = self.collect_daily_data(stock_code)
                if daily_data is not None:
                    stock_data['daily_data'] = daily_data
                    logger.info(f"[{stock_code}] âœ… ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    logger.warning(f"[{stock_code}] âŒ ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

                # ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘
                minute_data = self.collect_minute_data(stock_code, date_str)
                if minute_data is not None:
                    stock_data['minute_data'] = minute_data
                    logger.info(f"[{stock_code}] âœ… ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    logger.warning(f"[{stock_code}] âŒ ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

                # ì™„ì „ì„± ì²´í¬
                stock_data['complete'] = (stock_data['daily_data'] is not None and
                                        stock_data['minute_data'] is not None)

                dataset[date_str]['data'][stock_code] = stock_data

                if stock_data['complete']:
                    logger.info(f"[{stock_code}] âœ… ì™„ì „í•œ ë°ì´í„°ì…‹ ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    logger.warning(f"[{stock_code}] âš ï¸ ë¶ˆì™„ì „í•œ ë°ì´í„°ì…‹")

        # 4. ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
        self._print_collection_summary(dataset)

        return dataset

    def _print_collection_summary(self, dataset: Dict[str, Dict]):
        """ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½"""
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        logger.info(f"{'='*60}")

        total_stocks = 0
        complete_stocks = 0
        daily_only = 0
        minute_only = 0
        no_data = 0

        for date_str, date_data in dataset.items():
            date_total = len(date_data['data'])
            date_complete = sum(1 for stock_data in date_data['data'].values() if stock_data['complete'])
            date_daily_only = sum(1 for stock_data in date_data['data'].values()
                                if stock_data['daily_data'] is not None and stock_data['minute_data'] is None)
            date_minute_only = sum(1 for stock_data in date_data['data'].values()
                                 if stock_data['daily_data'] is None and stock_data['minute_data'] is not None)
            date_no_data = sum(1 for stock_data in date_data['data'].values()
                             if stock_data['daily_data'] is None and stock_data['minute_data'] is None)

            logger.info(f"{date_str}: ì „ì²´ {date_total}ê°œ, ì™„ì „ {date_complete}ê°œ, "
                       f"ì¼ë´‰ë§Œ {date_daily_only}ê°œ, ë¶„ë´‰ë§Œ {date_minute_only}ê°œ, ì—†ìŒ {date_no_data}ê°œ")

            total_stocks += date_total
            complete_stocks += date_complete
            daily_only += date_daily_only
            minute_only += date_minute_only
            no_data += date_no_data

        logger.info(f"{'-'*60}")
        logger.info(f"ğŸ“ˆ ì „ì²´ ì§‘ê³„:")
        logger.info(f"   ì´ ì¢…ëª©: {total_stocks}ê°œ")
        logger.info(f"   ì™„ì „í•œ ë°ì´í„°: {complete_stocks}ê°œ ({complete_stocks/total_stocks*100:.1f}%)")
        logger.info(f"   ì¼ë´‰ë§Œ: {daily_only}ê°œ ({daily_only/total_stocks*100:.1f}%)")
        logger.info(f"   ë¶„ë´‰ë§Œ: {minute_only}ê°œ ({minute_only/total_stocks*100:.1f}%)")
        logger.info(f"   ë°ì´í„° ì—†ìŒ: {no_data}ê°œ ({no_data/total_stocks*100:.1f}%)")
        logger.info(f"{'='*60}")

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    collector = AnalysisDataCollector()

    # 09/08-09/16 ê¸°ê°„ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    dataset = collector.collect_complete_dataset("20250908", "20250916")

    print(f"\nìˆ˜ì§‘ëœ ë‚ ì§œ: {list(dataset.keys())}")

if __name__ == "__main__":
    main()